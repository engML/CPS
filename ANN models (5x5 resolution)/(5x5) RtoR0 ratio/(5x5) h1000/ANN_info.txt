Model: "FFNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 25)]              0         
_________________________________________________________________
125_nodes_hidden_layer_1 (De (None, 50)                1300      
_________________________________________________________________
50_nodes_hidden_layer_2 (Den (None, 50)                2550      
_________________________________________________________________
25_nodes_hidden_layer_3 (Den (None, 25)                1275      
_________________________________________________________________
output_R_eff (Dense)         (None, 1)                 26        
=================================================================
Total params: 5,151
Trainable params: 5,151
Non-trainable params: 0
_________________________________________________________________

>>> training using 1700 data for a batch size of 32 in progress...

Epoch: 0, loss:0.3044,  mean_absolute_error:0.3783,  val_loss:0.0514,  val_mean_absolute_error:0.1807,  
....................................................................................................
Epoch: 100, loss:0.0002,  mean_absolute_error:0.0108,  val_loss:0.0005,  val_mean_absolute_error:0.0174,  
....................................................................................................
Epoch: 200, loss:0.0000,  mean_absolute_error:0.0051,  val_loss:0.0003,  val_mean_absolute_error:0.0139,  
....................................................................................................
Epoch: 300, loss:0.0000,  mean_absolute_error:0.0037,  val_loss:0.0003,  val_mean_absolute_error:0.0128,  
....................................................................................................
Epoch: 400, loss:0.0000,  mean_absolute_error:0.0025,  val_loss:0.0003,  val_mean_absolute_error:0.0122,  
....................................................................................................
Epoch: 500, loss:0.0000,  mean_absolute_error:0.0032,  val_loss:0.0003,  val_mean_absolute_error:0.0124,  
....................................................................................................
Epoch: 600, loss:0.0000,  mean_absolute_error:0.0032,  val_loss:0.0002,  val_mean_absolute_error:0.0118,  
....................................................................................................
Epoch: 700, loss:0.0000,  mean_absolute_error:0.0027,  val_loss:0.0002,  val_mean_absolute_error:0.0109,  
....................................................................................................
Epoch: 800, loss:0.0000,  mean_absolute_error:0.0020,  val_loss:0.0002,  val_mean_absolute_error:0.0107,  
....................................................................................................
Epoch: 900, loss:0.0000,  mean_absolute_error:0.0015,  val_loss:0.0002,  val_mean_absolute_error:0.0102,  
....................................................................................................
Epoch: 1000, loss:0.0000,  mean_absolute_error:0.0015,  val_loss:0.0002,  val_mean_absolute_error:0.0098,  
....................................................................................................
Epoch: 1100, loss:0.0000,  mean_absolute_error:0.0018,  val_loss:0.0002,  val_mean_absolute_error:0.0100,  
....................................................................................................
Epoch: 1200, loss:0.0000,  mean_absolute_error:0.0024,  val_loss:0.0002,  val_mean_absolute_error:0.0119,  
....................................................................................................
Epoch: 1300, loss:0.0000,  mean_absolute_error:0.0026,  val_loss:0.0001,  val_mean_absolute_error:0.0093,  
....................................................................................................
Epoch: 1400, loss:0.0000,  mean_absolute_error:0.0013,  val_loss:0.0001,  val_mean_absolute_error:0.0093,  
....................................................................................................
Epoch: 1500, loss:0.0000,  mean_absolute_error:0.0016,  val_loss:0.0001,  val_mean_absolute_error:0.0094,  
....................................................................................................
Epoch: 1600, loss:0.0000,  mean_absolute_error:0.0013,  val_loss:0.0002,  val_mean_absolute_error:0.0096,  
....................................................................................................
Epoch: 1700, loss:0.0000,  mean_absolute_error:0.0026,  val_loss:0.0001,  val_mean_absolute_error:0.0094,  
.........
--- the loss for the training data is [0.00013896806922275573, 0.008968028239905834]

>>> training using 1700 data for a batch size of 64 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0020,  val_loss:0.0001,  val_mean_absolute_error:0.0090,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0014,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
..
--- the loss for the training data is [0.00014593583182431757, 0.009280859492719173]

>>> training using 1700 data for a batch size of 128 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0010,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0013,  val_loss:0.0001,  val_mean_absolute_error:0.0092,  
....................................................................................................
Epoch: 200, loss:0.0000,  mean_absolute_error:0.0012,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
.......
--- the loss for the training data is [0.00014311129052657634, 0.009156894870102406]

>>> training using 1700 data for a batch size of 256 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0007,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0009,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
...........................................................
--- the loss for the training data is [0.00014239049050956964, 0.009150716476142406]

>>> training using 1700 data for a batch size of 512 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0003,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0005,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
....................................................
--- the loss for the training data is [0.0001410130353178829, 0.009087580256164074]

>>> training using 1700 data for a batch size of 1024 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0008,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0009,  val_loss:0.0001,  val_mean_absolute_error:0.0092,  
....................................................................................................
Epoch: 200, loss:0.0000,  mean_absolute_error:0.0001,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
.......................................................
--- the loss for the training data is [0.0001416487793903798, 0.009091042913496494]

>>> training using 1700 data for a batch size of 1700 in progress...

Epoch: 0, loss:0.0000,  mean_absolute_error:0.0007,  val_loss:0.0001,  val_mean_absolute_error:0.0092,  
....................................................................................................
Epoch: 100, loss:0.0000,  mean_absolute_error:0.0002,  val_loss:0.0001,  val_mean_absolute_error:0.0091,  
.....................................................
--- the loss for the training data is [0.00014295298024080694, 0.009146616794168949]
a batch size of 32 resulted in the best training loss of 0.00013896806922275573
Model: "FFNN"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input (InputLayer)           [(None, 25)]              0         
_________________________________________________________________
125_nodes_hidden_layer_1 (De (None, 50)                1300      
_________________________________________________________________
50_nodes_hidden_layer_2 (Den (None, 50)                2550      
_________________________________________________________________
25_nodes_hidden_layer_3 (Den (None, 25)                1275      
_________________________________________________________________
output_R_eff (Dense)         (None, 1)                 26        
=================================================================
Total params: 5,151
Trainable params: 5,151
Non-trainable params: 0
_________________________________________________________________

Test loss, accuracy: [0.00016057476750575006, 0.009224630892276764]
--- Test0 relative error: mean = 1.05%, standard deviation = 0.94%

Validation loss, accuracy: [0.00013896806922275573, 0.008968028239905834]
--- Validation0 relative error: mean = 1.03%, standard deviation = 0.85%

Train loss, accuracy: [5.453332505567232e-06, 0.0018890001811087132]
--- Train0 relative error: mean = 0.22%, standard deviation = 0.16%

Simulation time: 126.52 seconds.
